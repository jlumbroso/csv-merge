#!/usr/bin/env python3

# Python 2
from __future__ import print_function

import argparse
import collections
import glob
import logging
import os
import sys

import yaml
from QuotedCSV import read_csv

# Simpleeval library for calculated columns and expressions
import simpleeval

###############################################################################

DEFAULT_YAML = "config.yaml"

###############################################################################


class _Color:
    PURPLE = '\033[95m'
    CYAN = '\033[96m'
    DARKCYAN = '\033[36m'
    BLUE = '\033[94m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    END = '\033[0m'


class SimpleColorFormatter(logging.Formatter):
    _title = {
        "DEBUG": "{END}[{BOLD}DBUG{END}]{END}".format(**_Color.__dict__),
        "INFO": "{END}[{BOLD}{GREEN}INFO{END}]{END}".format(**_Color.__dict__),
        "ERROR": "{END}[{BOLD}{RED}ERR {END}]{END}".format(**_Color.__dict__),
        "WARNING": "{END}[{BOLD}{BLUE}WARN{END}]{END}".format(**_Color.__dict__)
    }

    def normalizePath(self, path):
        abs_path = os.path.abspath(path)
        pwd_path = os.getcwd()
        return abs_path.replace(pwd_path, ".", 1)

    def formatMessage(self, msg: logging.LogRecord):
        header = self._title.get(msg.levelname, self._title.get("INFO"))
        return("{} {} (\"{}\", line {}): {}".format(
            header,
            msg.module,
            self.normalizePath(msg.filename),
            msg.lineno,
            msg.message
        ))


def setupLogging(name=None, level="INFO"):

    # Add the color handler to the terminal output (sys.stderr)
    handler = logging.StreamHandler()
    formatter = SimpleColorFormatter()
    handler.setFormatter(formatter)

    # Set logging level
    root = logging.getLogger(name=name)
    root.setLevel(os.environ.get("LOGLEVEL", level))

    # Add the color handler to the logger
    if name == None:
        root.addHandler(handler)

    return root


logger = setupLogging()

###############################################################################
parser = argparse.ArgumentParser(
    description="""
    Merge the most recent version of several CSV files containing data on the
    same users, into one up-to-date CSV file, according to a YAML configuration
    file.
    """
)

parser.add_argument(
    "-y",
    help="name of the YAML job description.")

parser.add_argument(
    "-p",
    help="name of the YAML patch file (optional).")

parser.add_argument(
    "-o",
    help="output file name (optional, default /dev/stdout).",
    default="/dev/stdout")

parser.add_argument(
    "--verbose",
    action="store_true",
    help="display informational messages (info+debug output).")


def process_command_line(parser):
    logger.debug("Parsing command line input")

    args, _unknown = parser.parse_known_args()
    params = vars(args)

    if params["verbose"]:
        logger.info("Logger: Setting to DEBUG")
        logger.setLevel("DEBUG")

    logger.debug("Command Line: {}".format(params))

    yaml_file = params.get("y", DEFAULT_YAML) or DEFAULT_YAML

    candidates = []
    candidates.append(DEFAULT_YAML)
    candidates.append(yaml_file)
    candidates.append(os.path.join(yaml_file, DEFAULT_YAML))
    candidates = list(
        filter(os.path.isfile, filter(os.path.exists, candidates)))

    if len(candidates) == 0:
        logger.error(
            "Configuration File: File '{}' is invalid".format(yaml_file))
        logger.error("Call with --help for more information")
        sys.exit(1)

    if not yaml_file in candidates:
        yaml_file = candidates[0]

    logger.debug(
        "Configuration File: {}".format(yaml_file))

    config = dict()
    try:
        config = yaml.load(open(yaml_file))
    except Exception:
        logger.exception("Configuration File: Error while reading")
        logger.error("Call with --help for more information")

    logger.debug("Configuration File: {}".format(config))

    # Complete config with command-line parameters
    config["yaml"] = yaml_file
    if params.get("p", None):
        config["patch"] = params["p"]
        logger.debug(
            "Command Line: Patch param '{}'".format(params["p"]))

    if params.get("o", None):
        config["output"] = config.get("output", dict())
        config["output"]["path"] = config["output"].get("path", "")
        config["output"]["filename"] = params["o"]
        logger.debug(
            "Command Line: Output file param '{}'".format(params["o"]))

    logger.debug("Configuration: {}".format(config))

    return config


# Loading YAML file
config = process_command_line(parser)

# Configure some default properties (config variable is used throughout)

config["defaults"] = config.get("defaults", dict())

config["defaults"]["username"] = config["defaults"].get("username", "Username")
config["defaults"]["value"] = config["defaults"].get("value", "Value")
config["defaults"]["path"] = config["defaults"].get("path", "")

config["output"] = config.get("output", dict())

config["output"]["username"] = config["output"].get("username", config["defaults"]["username"])

# Processing files one by one

full_data = collections.OrderedDict()
full_headers = [config["output"]["username"]]

pattern_list = config.get("sources", dict()).keys()
counter = 0

DEFAULT_TYPE = "csv"
TYPES_FILE = [ "csv" ]

def extract_spec_internal_ref(username, record, item, maxs=dict()):
    weight = 1.0
    header = None
    value_available = True

    if isinstance(item, collections.abc.Mapping):
        weight = item.get("weight", weight)
        header = item.get("value", None)
    elif isinstance(item, str):
        header = item
    
    if not header:
        logger.debug(
            ("Include List: " +
                "Header for user '{}', item '{}' could not be computed").format(
                username, item
            ))
        return None

    if not header in maxs:
        logger.warning(
            ("Include List: " +
                "Header '{}' for user '{}', item '{}' has no normalization").format(
                header, username, item
            ))
        return None
    
    value = record.get(header)
    normalization = maxs.get(header)

    # Empty value, silently skip
    if not value or value == "":
        value_available = False
        value = "0.0"

    try:
        float_value = float(value)
        value = float_value
    except Exception:
        logger.debug("Could not float parse string '{}', assuming 0.0".format(value))
        value = 0.0
        value_available = False
    
    try:
        float_weight = float(weight)
        weight = float_weight
    except Exception:
        logger.debug("Could not float parse weight '{}', assuming 1.0".format(weight))
        weight = 1.0
    
    return {
        "username": username,
        "header": header,
        "value": value,
        "available": value_available,
        "normalization": normalization,
        "weight": weight,
    }

def process_source(name, specification, config):
    global counter, full_headers, full_data

    source_type = specification.get("type", DEFAULT_TYPE)

    logger.debug("Processing source '{}'".format(name))

    if source_type in TYPES_FILE:

        # ================================================================>
        # TYPE: Input File Source

        logger.debug("Type: Input File")

        fp = os.path.join(config["defaults"]["path"], name)

        results = glob.glob(fp)
        results.sort()

        logger.debug("Globbing '{}' yielded: {}.".format(fp, results))
        #logger.info(specification.get("include", list())[:1])
        if len(results) == 0:
            return

        # Last in lexicographic order
        last_result = results[-1]
        filename = last_result

        # Info
        logger.debug(">>> Reading {}".format(filename))

        if source_type == "csv":
            # ============================================================>
            # TYPE: CSV file

            logger.debug("Type: CSV Input File")

            # Get parameters
            counter += 1
            unique_header = "Header{:0<3}".format(counter)

            userkey = specification.get("username", config["defaults"]["username"])
            valuekey = specification.get("value", config["defaults"]["value"])
            header = specification.get("caption", unique_header)

            # Expand the header list
            full_headers.append(header)

            # Obtain data
            rows = read_csv(filename, as_dict=True)

            # Insert it into full-structure
            for row in rows:
                user = row[userkey]
                value = row[valuekey]

                if user.strip() == "":
                    continue

                full_data[user] = full_data.get(user, collections.OrderedDict())
                full_data[user][header] = value

    elif source_type == "range":
        # ================================================================>
        # TYPE: Calculated > Range

        logger.debug("Type: Range")

        input_field = specification.get("input", None)
        ranges = specification.get("ranges", list())

        computed_ranges = collections.OrderedDict()

        for (username, record) in full_data.items():
            if not input_field in record:
                continue
            
            try:
                input_value = float(record.get(input_field, None))
            except:
                continue
            
            logger.debug("* User '{}', computing range column '{}'".format(
                username, name))

            output_value = None
            for range_bracket in ranges:
                logger.debug("    - User '{}' assessing range {}".format(username, range_bracket))
                range_lb = range_bracket.get("lowerbound", 0.0)
                if input_value > range_lb:
                    output_value = range_bracket.get("value", None)
                    break

            logger.debug("=> User '{}', column '{}': {}".format(
                username, name, output_value))
            
            if output_value == None:
                continue

            computed_ranges[username] = output_value
        
        # Insert it into full-structure
        counter += 1
        unique_header = "Header{:0<3}".format(counter)
        header = specification.get("caption", unique_header)

        full_headers.append(header)

        for user, range_value in computed_ranges.items():
            full_data[user] = full_data.get(user, collections.OrderedDict())
            full_data[user][header] = range_value
        
        return



    elif source_type == "normalized-sum":

        # ================================================================>
        # TYPE: Calculated > Normalized Sum

        logger.debug("Type: Normalized Sum")

        # Compute maximums
        maxs = {}
        for row in full_data.values():
            if row == None or not isinstance(row, collections.abc.Mapping):
                continue
            for key, value in row.items():
                if value == "":
                    continue
                try:
                    f_value = float(value)
                except:
                    continue
                maxs[key] = max(f_value, maxs.get(key, 0.0))

        logger.debug("Detected Maxima: {}".format(maxs))

        computed_normalized_sums = collections.OrderedDict()

        for (username, record) in full_data.items():
            logger.debug(" - Evaluating '{}' for '{}' => {}".format(username, name, record))
            points_numerator = 0.0
            points_denominator = 0.0
            
            arg_partial = specification.get("partial", False)
            arg_prorata = specification.get("prorata", False)
            include_list = specification.get("include", list())
            extra_list = specification.get("extra", list())
            missing = False
            corrupt = False

            for item in include_list:
                
                row = extract_spec_internal_ref(
                    username=username,
                    record=record,
                    item=item,
                    maxs=maxs
                )
                logger.debug("   * Processing: {}".format(row))
                
                # Now we have everything to make our calculation
                if not row or not row.get("available", False):
                    if arg_prorata:
                        # When counting partial, don't penalize for unavailable values
                        continue
                    else:
                        missing = True
                        if not row:
                            logger.debug(
                                "=> User '{}', normalized sum '{}', item '{}': Corrupt".format(
                                    username, name, item
                                ))
                            corrupt = True
                            continue
                
                points_numerator += row["weight"] * row["value"]
                points_denominator += row["weight"] * row["normalization"]

            for item in extra_list:
                
                row = extract_spec_internal_ref(
                    username=username,
                    record=record,
                    item=item,
                    maxs=maxs
                )
                logger.debug("   * Processing Extra: {}".format(row))
                
                # Now we have everything to make our calculation
                if not row or not row["available"]:
                    continue
                
                points_numerator += row["weight"] * row["value"]

            logger.debug("=> User '{}', normalized sum '{}': {} / {}".format(
                username, name, points_numerator, points_denominator))

            if corrupt or points_denominator < 0.9:
                logger.debug("=> User '{}', normalized sum '{}': Not available".format(username, name))
                continue
            
            if missing and not arg_partial:
                logger.debug("=> User '{}', normalized sum '{}': Missing values, skipping".format(username, name))
                continue

            # Compute total
            try:
                normalized_sum = float(points_numerator/points_denominator)
            except:
                logger.debug("Unexpected error while computing normalized sum")
                raise

            logger.debug("=> User '{}', normalized sum '{}': {}".format(
                username, name, normalized_sum
            ))

            computed_normalized_sums[username] = normalized_sum
        
        # Insert it into full-structure
        counter += 1
        unique_header = "Header{:0<3}".format(counter)
        header = specification.get("caption", unique_header)

        full_headers.append(header)

        for user, normalized_sum in computed_normalized_sums.items():
            full_data[user] = full_data.get(user, collections.OrderedDict())
            full_data[user][header] = "{:.2f}".format((normalized_sum)*100.0)
        
        return

    else:

        # Unsupported data type
        return

for (name, specification) in config.get("sources", dict()).items():
    process_source(name=name, specification=specification, config=config)
    

# Patch the data
patch_pattern = config.get("patch")
if patch_pattern:
    patch_files = glob.glob(patch_pattern)
    logger.debug("Candidates for patch file: {}".format(patch_files))
    patch_file = patch_files[-1]
    logger.debug(
        "Running last one by lexicographic order: {}".format(patch_file))
    rows = read_csv(patch_file, as_dict=True)

    logger.debug("Patch: {}".format(rows))

    for patch in rows:
        user = patch.get("Username", None)
        header = patch.get("Caption", None)
        patched_value = patch.get("Value", None)

        if user == None or header == None or patched_value == None:
            continue

        prev_value = full_data.get(user, dict()).get(header, None)

        if not user in full_data:
            logger.debug("Adding new user '{}' by patch".format(user))
            full_data[user] = dict()

        full_data[user][header] = patched_value
        logger.debug("Patch: data [{}.{}] {} => {}".format(
            user,
            header,
            prev_value,
            patched_value
        ))

# Output the aggregated data
output_filename = os.path.join(
    config["output"].get("path", ""),
    config["output"].get("filename", "/dev/stdout"))

logger.debug("Output path: {}".format(output_filename))

try:
    with open(output_filename, "w") as f:
        f.write(",".join(full_headers))
        f.write("\n")
        for user in full_data.keys():
            userdata = full_data[user]
            userdata[config["output"]["username"]] = user
            f.write(",".join(map(lambda colname: userdata.get(colname, ""), full_headers)))
            f.write("\n")

except BrokenPipeError:
    logger.warning("Data could not be piped to next program because of a pipe error")
